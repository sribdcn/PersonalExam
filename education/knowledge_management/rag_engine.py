# -*- coding: utf-8 -*-
"""
Copyright (c) 2025 AIç³»ç»ŸåŠåº”ç”¨è¯¾é¢˜ç»„@SRIBD

Personalized Question Generation System Based on LLM and Knowledge Graph Collaboration

æœ¬åœ°RAGå¼•æ“ - åŸºäºå‘é‡æ£€ç´¢å’ŒçŸ¥è¯†å›¾è°±
"""

import logging
import numpy as np
import json
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import re

logger = logging.getLogger(__name__)


class LocalRAGEngine:
    """æœ¬åœ°RAGå¼•æ“ - å‘é‡æ£€ç´¢ + çŸ¥è¯†å›¾è°±"""
    
    def __init__(self, embedding_model, llm_model):
        """
        
        Args:
            embedding_model: BGEåµŒå…¥æ¨¡å‹
            llm_model: ç›˜å¤7Bæ¨¡å‹
        """
        self.embedding_model = embedding_model
        self.llm_model = llm_model
        
        # é¢˜ç›®ç´¢å¼•
        self.question_texts = []  # é¢˜ç›®æ–‡æœ¬åˆ—è¡¨
        self.question_embeddings = None  # é¢˜ç›®åµŒå…¥çŸ©é˜µ
        self.question_metadata = []  # é¢˜ç›®å…ƒæ•°æ®
        
        logger.info("âœ… æœ¬åœ°RAGå¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def build_question_index(self, questions: List[Dict[str, Any]]):

        logger.info(f"ğŸ”„ æ­£åœ¨ä¸º {len(questions)} é“é¢˜ç›®æ„å»ºå‘é‡ç´¢å¼•...")
        
        self.question_texts = []
        self.question_metadata = []
        
        for q in questions:
            # æ„å»ºé¢˜ç›®çš„æ–‡æœ¬è¡¨ç¤º
            text = self._format_question_for_indexing(q)
            self.question_texts.append(text)
            
            # ä¿å­˜å…ƒæ•°æ®
            self.question_metadata.append({
                'question': q,
                'major_point': q.get('çŸ¥è¯†ç‚¹å¤§ç±»', ''),
                'minor_point': q.get('çŸ¥è¯†ç‚¹å°ç±»', ''),
                'difficulty': q.get('éš¾åº¦', 0.5),
                'id': q.get('é¢˜å·', len(self.question_metadata))
            })
        
        # æ‰¹é‡è®¡ç®—åµŒå…¥
        logger.info("ğŸ”„ æ­£åœ¨è®¡ç®—é¢˜ç›®åµŒå…¥...")
        self.question_embeddings = self.embedding_model.encode(
            self.question_texts,
            normalize=True
        )
        
        logger.info(f"âœ… é¢˜ç›®ç´¢å¼•æ„å»ºå®Œæˆ: {len(self.question_texts)} é“é¢˜, "
                   f"åµŒå…¥ç»´åº¦ {self.question_embeddings.shape[1]}")
    
    def _format_question_for_indexing(self, question: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–é¢˜ç›®ç”¨äºç´¢å¼•"""
        major = question.get('çŸ¥è¯†ç‚¹å¤§ç±»', '')
        minor = question.get('çŸ¥è¯†ç‚¹å°ç±»', '')
        problem = question.get('é—®é¢˜', '')
        answer = question.get('ç­”æ¡ˆ', '')
        explanation = question.get('è§£æ', '')
        
        # ç»„åˆå…³é”®ä¿¡æ¯
        text = f"çŸ¥è¯†ç‚¹ï¼š{major} {minor}\né—®é¢˜ï¼š{problem}\nç­”æ¡ˆï¼š{answer}\nè§£æï¼š{explanation}"
        return text
    
    def search_questions(self, query: str, 
                        major_point: Optional[str] = None,
                        minor_point: Optional[str] = None,
                        difficulty_range: Optional[Tuple[float, float]] = None,
                        top_k: int = 5) -> List[Dict[str, Any]]:

        if self.question_embeddings is None:
            logger.error("âŒ é¢˜ç›®ç´¢å¼•æœªæ„å»º")
            return []
        
        # è®¡ç®—æŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_model.encode([query], normalize=True)[0]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(self.question_embeddings, query_embedding)
        
        # è·å–å€™é€‰é¢˜ç›®
        candidates = []
        for idx, score in enumerate(similarities):
            metadata = self.question_metadata[idx]
            
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            if major_point and metadata['major_point'] != major_point:
                continue
            if minor_point and metadata['minor_point'] != minor_point:
                continue
            if difficulty_range:
                diff = metadata['difficulty']
                if not (difficulty_range[0] <= diff < difficulty_range[1]):
                    continue
            
            candidates.append({
                'question': metadata['question'],
                'score': float(score),
                'metadata': metadata
            })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        candidates.sort(key=lambda x: x['score'], reverse=True)
        
        # è¿”å›top_k
        results = candidates[:top_k]
        
        logger.info(f"ğŸ” æ£€ç´¢åˆ° {len(results)} é“ç›¸å…³é¢˜ç›® (query: '{query[:50]}...')")
        return results
    
    def extract_entities_and_relations(self, text_context: str) -> Dict[str, Any]:

        prompt = f"""åˆ†æä»¥ä¸‹æ•°å­¦é¢˜ç›®ï¼Œæå–å…³é”®çš„çŸ¥è¯†ç‚¹å®ä½“ã€‚

é¢˜ç›®å†…å®¹ï¼š
{text_context[:1000]}

è¦æ±‚ï¼š
1. æå–3-5ä¸ªæ ¸å¿ƒæ•°å­¦çŸ¥è¯†ç‚¹
2. æ¯ä¸ªçŸ¥è¯†ç‚¹ç”¨ä¸€ä¸ªè¯æˆ–çŸ­è¯­è¡¨ç¤º
3. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONï¼š

{{
  "entities": [
    {{"name": "ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹", "type": "çŸ¥è¯†ç‚¹"}},
    {{"name": "å› å¼åˆ†è§£", "type": "æ–¹æ³•"}}
  ],
  "relations": [
    {{"source": "ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹", "target": "å› å¼åˆ†è§£", "relation": "å¯ä»¥ä½¿ç”¨"}}
  ]
}}

åªè¾“å‡ºJSONï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šæ–‡å­—ã€‚
"""
        
        try:
            # ç¡®ä¿ç›˜å¤7Bå·²åŠ è½½
            if not self.llm_model.is_loaded:
                logger.info("ğŸ”„ åŠ è½½ç›˜å¤7Bæ¨¡å‹...")
                self.llm_model.load_model()
            
            # ç”Ÿæˆï¼ˆé™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„JSONï¼‰
            response = self.llm_model.generate(prompt, temperature=0.1, max_length=1024)
            
            # è§£æJSON
            kg_data = self._parse_kg_response(response)
            
            logger.info(f"âœ… æå–åˆ° {len(kg_data.get('entities', []))} ä¸ªå®ä½“, "
                       f"{len(kg_data.get('relations', []))} ä¸ªå…³ç³»")
            
            return kg_data
            
        except Exception as e:
            logger.error(f"âŒ å®ä½“å…³ç³»æå–å¤±è´¥: {e}")
            return {'entities': [], 'relations': []}
    
    def _parse_kg_response(self, response: str) -> Dict[str, Any]:
        """è§£æçŸ¥è¯†å›¾è°±å“åº”"""
        try:
            # æŸ¥æ‰¾JSONéƒ¨åˆ†
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                return {'entities': [], 'relations': []}
            
            json_str = response[start_idx:end_idx]
            kg_data = json.loads(json_str)
            
            return kg_data
        except Exception as e:
            logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
            
            # å°è¯•æ­£åˆ™æå–
            entities = []
            relations = []
            
            # æå–å®ä½“
            entity_pattern = r'å®ä½“[:ï¼š]\s*([^\n]+)'
            for match in re.finditer(entity_pattern, response):
                entities.append({'name': match.group(1).strip(), 'type': 'æ¦‚å¿µ'})
            
            # æå–å…³ç³»
            relation_pattern = r'å…³ç³»[:ï¼š]\s*([^\n]+)'
            for match in re.finditer(relation_pattern, response):
                relations.append({'source': '', 'target': '', 'relation': match.group(1).strip()})
            
            return {'entities': entities, 'relations': relations}
    
    def build_knowledge_subgraph(self, 
                                student_mastery: float,
                                major_point: str,
                                minor_point: str,
                                top_k: int = 5) -> Dict[str, Any]:
        """
        æ„å»ºçŸ¥è¯†å­å›¾
        
        Args:
            student_mastery: å­¦ç”ŸæŒæ¡åº¦
            major_point: çŸ¥è¯†ç‚¹å¤§ç±»
            minor_point: çŸ¥è¯†ç‚¹å°ç±»
            top_k: æ£€ç´¢é¢˜ç›®æ•°é‡
            
        Returns:
            çŸ¥è¯†å­å›¾æ•°æ®
        """
        logger.info(f"ğŸ”„ æ„å»ºçŸ¥è¯†å­å›¾: {major_point}/{minor_point}, æŒæ¡åº¦: {student_mastery:.3f}")
        
        # 1. æ„å»ºæŸ¥è¯¢
        if student_mastery < 0.3:
            difficulty_desc = "ç®€å• åŸºç¡€"
        elif student_mastery < 0.7:
            difficulty_desc = "ä¸­ç­‰"
        else:
            difficulty_desc = "å›°éš¾ æé«˜"
        
        query = f"{major_point} {minor_point} {difficulty_desc}"
        
        # 2. æ£€ç´¢ç›¸å…³é¢˜ç›®
        retrieved_questions = self.search_questions(
            query=query,
            major_point=major_point,
            minor_point=minor_point,
            top_k=top_k
        )
        
        if not retrieved_questions:
            logger.warning("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³é¢˜ç›®")
            return {
                'retrieved_questions': [],
                'entities': [],
                'relations': [],
                'context': ''
            }
        
        # 3. æ„å»ºä¸Šä¸‹æ–‡
        context_texts = []
        for item in retrieved_questions:
            q = item['question']
            text = f"""é¢˜ç›®{q.get('é¢˜å·', '')}:
çŸ¥è¯†ç‚¹: {q.get('çŸ¥è¯†ç‚¹å¤§ç±»', '')} / {q.get('çŸ¥è¯†ç‚¹å°ç±»', '')}
éš¾åº¦: {q.get('éš¾åº¦', 0.5)}
é—®é¢˜: {q.get('é—®é¢˜', '')}
ç­”æ¡ˆ: {q.get('ç­”æ¡ˆ', '')}
è§£æ: {q.get('è§£æ', '')}
"""
            context_texts.append(text)
        
        full_context = "\n\n".join(context_texts)
        
        # 4. æå–å®ä½“å’Œå…³ç³»
        kg_data = self.extract_entities_and_relations(full_context)
        
        # 5. ç»„åˆç»“æœ
        subgraph = {
            'retrieved_questions': retrieved_questions,
            'entities': kg_data.get('entities', []),
            'relations': kg_data.get('relations', []),
            'context': full_context,
            'student_mastery': student_mastery,
            'target_knowledge': f"{major_point}/{minor_point}"
        }
        
        logger.info(f"âœ… çŸ¥è¯†å­å›¾æ„å»ºå®Œæˆ: {len(retrieved_questions)} é“é¢˜, "
                   f"{len(subgraph['entities'])} ä¸ªå®ä½“")
        
        return subgraph
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_questions': len(self.question_texts),
            'embedding_dim': self.question_embeddings.shape[1] if self.question_embeddings is not None else 0,
            'indexed': self.question_embeddings is not None
        }


def create_rag_engine(embedding_model, llm_model) -> LocalRAGEngine:
    """åˆ›å»ºRAGå¼•æ“"""
    return LocalRAGEngine(embedding_model, llm_model)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import sys
    sys.path.append("..")
    from config import BGE_M3_MODEL_PATH, PANGU_MODEL_PATH, EMBEDDING_MODEL_CONFIG, PANGU_MODEL_CONFIG
    from models.embedding_model import create_embedding_model
    from models.llm_models import create_llm_model
    
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºæ¨¡å‹
    embedding_model = create_embedding_model(BGE_M3_MODEL_PATH, EMBEDDING_MODEL_CONFIG)
    llm_model = create_llm_model('pangu', PANGU_MODEL_PATH, PANGU_MODEL_CONFIG)
    
    # åˆ›å»ºRAGå¼•æ“
    rag = create_rag_engine(embedding_model, llm_model)
    
    # æµ‹è¯•é¢˜ç›®
    test_questions = [
        {
            'é¢˜å·': 1,
            'é—®é¢˜': 'x^2 - 5x + 6 = 0',
            'ç­”æ¡ˆ': 'x = 2 æˆ– x = 3',
            'éš¾åº¦': 0.3,
            'çŸ¥è¯†ç‚¹å¤§ç±»': 'ä»£æ•°',
            'çŸ¥è¯†ç‚¹å°ç±»': 'ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹',
            'è§£æ': 'å› å¼åˆ†è§£å¾— (x-2)(x-3)=0'
        }
    ]
    
    # æ„å»ºç´¢å¼•
    rag.build_question_index(test_questions)
    
    # æµ‹è¯•æ£€ç´¢
    results = rag.search_questions("äºŒæ¬¡æ–¹ç¨‹", major_point="ä»£æ•°", top_k=3)
    print(f"æ£€ç´¢ç»“æœ: {len(results)} é“é¢˜")
    
    # æµ‹è¯•çŸ¥è¯†å­å›¾
    subgraph = rag.build_knowledge_subgraph(0.5, "ä»£æ•°", "ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹")
    print(f"çŸ¥è¯†å­å›¾: {len(subgraph['entities'])} ä¸ªå®ä½“")