# -*- coding: utf-8 -*-
"""
Copyright (c) 2025 AIç³»ç»ŸåŠåº”ç”¨è¯¾é¢˜ç»„@SRIBD

åŸºäºLLMå’ŒçŸ¥è¯†å›¾è°±ååŒçš„ä¸ªæ€§åŒ–å‡ºé¢˜ç³»ç»Ÿ (PersonalExam)
Personalized Question Generation System Based on LLM and Knowledge Graph Collaboration

LLMæ¨¡å‹æ¥å£æ¨¡å—
ç»Ÿä¸€ä½¿ç”¨PanGu-7Bæ¨¡å‹ï¼Œæ”¯æŒæ˜‡è…¾NPUåŠ é€Ÿå’Œå¤šNPUå¹¶è¡Œ
ä¼˜åŒ–ï¼šå•ä¾‹æ¨¡å¼é¿å…é‡å¤åŠ è½½ï¼Œæ”¯æŒå¤šNPUè´Ÿè½½å‡è¡¡
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Optional, Dict, Any
import logging
import re
import threading

logger = logging.getLogger(__name__)

# å…¨å±€æ¨¡å‹å®ä¾‹ç¼“å­˜(å•ä¾‹æ¨¡å¼)
_MODEL_INSTANCE_CACHE = {}
_CACHE_LOCK = threading.Lock()


class PanGuModel:
    """ç›˜å¤7Bæ¨¡å‹å°è£… - ç»Ÿä¸€ç”¨äºå‡ºé¢˜å’Œè¯„ä¼°"""
    
    def __init__(self, model_path: str, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–ç›˜å¤7Bæ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            config: æ¨¡å‹é…ç½®å­—å…¸
        """
        self.model_path = model_path
        self.config = config
        
        # æ£€æŸ¥NPUå¯ç”¨æ€§å¹¶é…ç½®å¤šNPU
        self.devices = self._setup_devices(config.get("device", "npu"))
        self.current_device_idx = 0  # ç”¨äºè½®è¯¢è´Ÿè½½å‡è¡¡
        
        logger.info(f"ç›˜å¤7Bæ¨¡å‹è®¾å¤‡é…ç½®: {self.devices}")
        
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
        
        # ç›˜å¤7Bç‰¹æ®Šé…ç½®
        self.system_prompt = config.get("system_prompt", "")
        self.eos_token_id = config.get("eos_token_id", 45892)
        self.enable_thinking = config.get("enable_thinking", False)
        
    def _setup_devices(self, device_config: str) -> list:
        """è®¾ç½®è®¡ç®—è®¾å¤‡ï¼Œæ”¯æŒå¤šNPU"""
        if device_config == "npu":
            try:
                import torch_npu
                if torch.npu.is_available():
                    npu_count = torch.npu.device_count()
                    logger.info(f"âœ… æ£€æµ‹åˆ° {npu_count} ä¸ªNPUè®¾å¤‡")
                    
                    # è¿”å›æ‰€æœ‰å¯ç”¨NPUè®¾å¤‡åˆ—è¡¨
                    devices = [f"npu:{i}" for i in range(npu_count)]
                    
                    # æ˜¾ç¤ºæ¯ä¸ªNPUçš„æ˜¾å­˜ä¿¡æ¯
                    for i in range(npu_count):
                        props = torch.npu.get_device_properties(i)
                        total_memory = props.total_memory / (1024**3)  # è½¬æ¢ä¸ºGB
                        logger.info(f"  NPU {i}: {props.name}, æ€»æ˜¾å­˜: {total_memory:.2f} GB")
                    
                    return devices
                else:
                    logger.error("âŒ NPUä¸å¯ç”¨")
                    raise RuntimeError("NPUä¸å¯ç”¨ï¼Œç³»ç»Ÿç»ˆæ­¢")
            except ImportError:
                logger.error("âŒ torch_npuæœªå®‰è£…")
                raise RuntimeError("torch_npuæœªå®‰è£…ï¼Œç³»ç»Ÿç»ˆæ­¢")
        elif device_config == "cuda" and torch.cuda.is_available():
            return ["cuda:0"]
        else:
            logger.error("âŒ ä¸æ”¯æŒçš„è®¾å¤‡é…ç½®æˆ–è®¾å¤‡ä¸å¯ç”¨")
            raise RuntimeError("è®¾å¤‡ä¸å¯ç”¨ï¼Œç³»ç»Ÿç»ˆæ­¢")
    
    def _get_next_device(self) -> str:
        if len(self.devices) == 1:
            return self.devices[0]
        
        # è½®è¯¢æ–¹å¼é€‰æ‹©è®¾å¤‡
        device = self.devices[self.current_device_idx]
        self.current_device_idx = (self.current_device_idx + 1) % len(self.devices)
        return device
            
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer - åªåŠ è½½ä¸€æ¬¡"""
        if self.is_loaded:
            logger.info("âœ… æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤åŠ è½½")
            return
        
        try:
            logger.info(f"ğŸ”„ æ­£åœ¨åŠ è½½ç›˜å¤7Bæ¨¡å‹: {self.model_path}")
            
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                use_fast=False,
                trust_remote_code=True,
                local_files_only=False
            )
            
            # è®¾ç½®æ•°æ®ç±»å‹
            torch_dtype = torch.float16
            
            # å¤šNPUç­–ç•¥
            if len(self.devices) > 1:
                logger.info(f"ğŸš€ ä½¿ç”¨å¤šNPUæ¨¡å¼ï¼Œè·¨{len(self.devices)}ä¸ªè®¾å¤‡åˆ†å¸ƒæ¨¡å‹")
                
                try:
                    # æ–¹æ¡ˆ1: ä½¿ç”¨accelerateçš„è‡ªåŠ¨è®¾å¤‡æ˜ å°„
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        trust_remote_code=True,
                        torch_dtype=torch_dtype,
                        device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°å¤šä¸ªNPU
                        max_memory={i: "50GB" for i in range(len(self.devices))},  # æ¯ä¸ªNPUæœ€å¤§ä½¿ç”¨50GB
                        local_files_only=False,
                        low_cpu_mem_usage=True  # å‡å°‘CPUå†…å­˜ä½¿ç”¨
                    )
                    logger.info("âœ… ä½¿ç”¨device_map='auto'æ¨¡å¼æˆåŠŸåŠ è½½")
                except Exception as e:
                    logger.warning(f"âš ï¸  device_map='auto'åŠ è½½å¤±è´¥: {e}")
                    logger.info("ğŸ“ å›é€€åˆ°å•NPUæ¨¡å¼")
                    
                    # å›é€€åˆ°å•NPUæ¨¡å¼
                    import torch_npu
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        trust_remote_code=True,
                        torch_dtype=torch_dtype,
                        local_files_only=False
                    )
                    self.model = self.model.to(self.devices[0])
                    self.devices = [self.devices[0]]  # æ›´æ–°ä¸ºå•è®¾å¤‡
            else:
                # å•NPUæ¨¡å¼
                logger.info(f"ğŸ“ ä½¿ç”¨å•NPUæ¨¡å¼: {self.devices[0]}")
                import torch_npu
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    trust_remote_code=True,
                    torch_dtype=torch_dtype,
                    local_files_only=False
                )
                self.model = self.model.to(self.devices[0])
            
            self.model.eval()
            self.is_loaded = True
            
            # æ˜¾ç¤ºæ¨¡å‹åˆ†å¸ƒæƒ…å†µ
            if hasattr(self.model, 'hf_device_map'):
                logger.info(f"ğŸ“Š æ¨¡å‹åˆ†å¸ƒæƒ…å†µ:")
                device_summary = {}
                for layer, device in self.model.hf_device_map.items():
                    device_summary[device] = device_summary.get(device, 0) + 1
                for device, count in sorted(device_summary.items()):
                    logger.info(f"   {device}: {count} å±‚")
            else:
                logger.info(f"ğŸ“Š æ¨¡å‹ä½äº: {self.devices[0]}")
            
            logger.info(f"âœ… ç›˜å¤7Bæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ ç›˜å¤7Bæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def _build_chat_prompt(self, user_prompt: str, enable_thinking: bool = None) -> str:
        """
        æ„å»ºç¬¦åˆç›˜å¤7Bæ ¼å¼çš„å¯¹è¯æç¤ºè¯
        
        Args:
            user_prompt: ç”¨æˆ·è¾“å…¥
            enable_thinking: æ˜¯å¦å¯ç”¨æ€ç»´é“¾
            
        Returns:
            æ ¼å¼åŒ–çš„æç¤ºè¯
        """
        if enable_thinking is None:
            enable_thinking = self.enable_thinking
        
        # æ ¹æ®æ˜¯å¦å¯ç”¨æ€ç»´é“¾æ·»åŠ ç‰¹æ®Šæ ‡è®°
        thinking_flag = " /auto_think" if enable_thinking else " /no_think"
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt + thinking_flag}
        ]
        
        # ä½¿ç”¨tokenizerçš„chat template
        try:
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            return prompt
        except Exception as e:
            logger.warning(f"åº”ç”¨chat templateå¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•æ ¼å¼")
            return f"{self.system_prompt}\n\nUser: {user_prompt}\n\nAssistant:"
    
    def _parse_pangu_output(self, output_text: str) -> str:

        try:
            # ç›˜å¤7Bä½¿ç”¨ç‰¹æ®Štokenåˆ†éš”æ€ç»´å’Œè¾“å‡º
            if "[unused17]" in output_text:
                # æå–æ€ç»´å†…å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if "[unused16]" in output_text:
                    thinking_content = output_text.split("[unused17]")[0].split("[unused16]")[-1].strip()
                    if thinking_content:
                        logger.debug(f"æ€ç»´è¿‡ç¨‹: {thinking_content[:100]}...")
                
                # æå–å®é™…è¾“å‡º
                content = output_text.split("[unused17]")[-1].split("[unused10]")[0].strip()
                return content
            else:
                # æ²¡æœ‰ç‰¹æ®Štokenï¼Œç›´æ¥è¿”å›æ¸…ç†åçš„æ–‡æœ¬
                content = output_text.replace("[unused16]", "").replace("[unused17]", "").replace("[unused10]", "").strip()
                return content
                
        except Exception as e:
            logger.warning(f"è§£æç›˜å¤è¾“å‡ºå¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹æ–‡æœ¬")
            return output_text
            
    def generate(self, prompt: str, 
                max_length: Optional[int] = None,
                temperature: Optional[float] = None,
                top_p: Optional[float] = None,
                enable_thinking: Optional[bool] = None) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            top_p: top_pé‡‡æ ·å‚æ•°
            enable_thinking: æ˜¯å¦å¯ç”¨æ€ç»´é“¾
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        if not self.is_loaded:
            self.load_model()
            
        max_new_tokens = max_length or self.config.get("max_new_tokens", 4096)
        temperature = temperature or self.config.get("temperature", 0.7)
        top_p = top_p or self.config.get("top_p", 0.9)
        
        try:
            # æ„å»ºæç¤ºè¯
            formatted_prompt = self._build_chat_prompt(prompt, enable_thinking)
            
            # Tokenize
            model_inputs = self.tokenizer(formatted_prompt, return_tensors="pt")
            
            # æ ¹æ®æ¨¡å‹åŠ è½½æ–¹å¼å†³å®šå¦‚ä½•å¤„ç†è¾“å…¥
            if hasattr(self.model, 'hf_device_map'):
                # å¤šNPUæ¨¡å¼ï¼ˆä½¿ç”¨device_map="auto"ï¼‰
                # å°†è¾“å…¥ç§»åŠ¨åˆ°ç¬¬ä¸€ä¸ªæ¨¡å‹å±‚æ‰€åœ¨çš„è®¾å¤‡
                first_device = list(self.model.hf_device_map.values())[0]
                model_inputs = {k: v.to(first_device) for k, v in model_inputs.items()}
            else:
                # å•NPUæ¨¡å¼
                import torch_npu
                model_inputs = {k: v.to(self.devices[0]) for k, v in model_inputs.items()}
            
            # ç”Ÿæˆï¼ˆä¼˜åŒ–ï¼šä½æ¸©åº¦æ—¶å…³é—­é‡‡æ ·ä»¥æå‡é€Ÿåº¦ï¼‰
            with torch.no_grad():
                # å½“temperatureå¾ˆä½æ—¶ï¼Œä½¿ç”¨è´ªå¿ƒè§£ç ï¼ˆæ›´å¿«ï¼‰
                do_sample = temperature > 0.1
                outputs = self.model.generate(
                    **model_inputs,
                    max_new_tokens=min(max_new_tokens, 512),  # é™åˆ¶æœ€å¤§é•¿åº¦ä»¥æå‡é€Ÿåº¦
                    temperature=temperature if do_sample else None,
                    top_p=top_p if do_sample else None,
                    do_sample=do_sample,
                    eos_token_id=self.eos_token_id,
                    pad_token_id=self.tokenizer.eos_token_id,
                    return_dict_in_generate=True
                )
            
            # è§£ç 
            input_length = model_inputs['input_ids'].shape[1]
            generated_tokens = outputs.sequences[:, input_length:]
            
            # ç§»å›CPUè¿›è¡Œè§£ç 
            generated_tokens = generated_tokens.cpu()
            
            output_text = self.tokenizer.decode(generated_tokens[0], skip_special_tokens=False)
            
            # è§£æè¾“å‡º
            response = self._parse_pangu_output(output_text)
            
            logger.debug(f"âœ… ç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(response)}")
            return response
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise RuntimeError(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'model') and self.model is not None:
            del self.model
        if hasattr(self, 'tokenizer') and self.tokenizer is not None:
            del self.tokenizer
        
        # æ¸…ç†NPUç¼“å­˜
        try:
            import torch_npu
            if torch.npu.is_available():
                for device in self.devices:
                    torch.npu.empty_cache()
        except:
            pass


def create_llm_model(model_type: str, model_path: str, config: Dict[str, Any]) -> PanGuModel:
    import os
    
    # ä½¿ç”¨å•ä¾‹æ¨¡å¼ï¼Œé¿å…é‡å¤åŠ è½½
    cache_key = f"{model_type}_{model_path}"
    
    with _CACHE_LOCK:
        if cache_key in _MODEL_INSTANCE_CACHE:
            logger.info(f"â™»ï¸  è¿”å›å·²ç¼“å­˜çš„æ¨¡å‹å®ä¾‹: {model_type}")
            return _MODEL_INSTANCE_CACHE[cache_key]
        
        # ç»Ÿä¸€ä½¿ç”¨ç›˜å¤7Bæ¨¡å‹
        logger.info(f"ğŸ†• åˆ›å»ºæ–°çš„ç›˜å¤7Bæ¨¡å‹å®ä¾‹ï¼ˆä»»åŠ¡ç±»å‹: {model_type}ï¼‰")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not os.path.exists(model_path):
            logger.error(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            raise RuntimeError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        
        model = PanGuModel(model_path, config)
        
        # ç¼“å­˜å®ä¾‹
        _MODEL_INSTANCE_CACHE[cache_key] = model
        
        return model


def clear_model_cache():
    """æ¸…ç†æ¨¡å‹ç¼“å­˜ï¼ˆç”¨äºé‡æ–°åŠ è½½æ¨¡å‹ï¼‰"""
    global _MODEL_INSTANCE_CACHE
    with _CACHE_LOCK:
        for model in _MODEL_INSTANCE_CACHE.values():
            del model
        _MODEL_INSTANCE_CACHE.clear()
        logger.info("ğŸ—‘ï¸  æ¨¡å‹ç¼“å­˜å·²æ¸…ç†")


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import sys
    sys.path.append("..")
    from config import PANGU_MODEL_PATH, PANGU_MODEL_CONFIG
    
    logging.basicConfig(level=logging.INFO)
    
    # æµ‹è¯•ç›˜å¤æ¨¡å‹
    pangu = create_llm_model('pangu', PANGU_MODEL_PATH, PANGU_MODEL_CONFIG)
    pangu.load_model()
    
    test_prompt = "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹è´å¶æ–¯å®šç†"
    response = pangu.generate(test_prompt)
    print(f"å“åº”: {response}")